++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++ 
+++ VORA 1.3 SINGLE NODE INSTALL NOTES +++
++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++ 

#######
# AWS #
#######

# make sure you launch suse-sles-11-sp4-sapcal-v20160415-hvm-ssd-x86_64 AMI in AWS #

#######################
# LOGGING IN WITH MAC #
#######################

ls $HOME/.ssh/
cp Desktop/Vora13.pem.txt $HOME/.ssh/Vora13.pem
chmod 400 /Users/Bob/.ssh/Vora13.pem
ssh -i /Users/Bob/.ssh/Vora13.pem ec2-user@12.34.56.78

##################
# LINUX COMMANDS #
##################

sudo su -
/etc/init.d/ntp restart

wget https://www.dropbox.com/s/big19drvsxz236p/SLES11-compat-c%2B%2B.tar?dl=0 -O SLES11-compat-c++.tar
ls

mkdir tmp
tar -xf SLES11-compat-c++.tar -C tmp
sudo zypper -n install tmp/*rpm

# prepare dlog
zypper install libaio
cat /proc/sys/fs/file-max
echo "fs.file-max=983040" | tee -a /etc/sysctl.conf
sysctl -p
cat /etc/security/limits.conf
echo "*                -       nofile          1000000" | tee -a /etc/security/limits.conf
cat /etc/security/limits.conf
locale
export LC_ALL=en_US.UTF-8
locale
# prepare document store server (nothing to do on AWS image) #
zypper install numactl
# prepare disk engine server (nothing to do on AWS image) #
zypper install libtool
# prepare cluster manager (nothing to do on AWS image) #
sudo zypper install lsof
exit
sudo su -

chmod 700 /etc/sudoers
echo "%sysadmin ALL=(ALL) NOPASSWD:ALL" | tee -a /etc/sudoers
chmod 440 /etc/sudoers
groupadd sysadmin
/usr/sbin/useradd -m -g users -G sysadmin cluster_admin
su - cluster_admin

ssh-keygen -t rsa
PRESS RETURN
PRESS RETURN

chmod 700 ~/
chmod 700 ~/.ssh
cat ~/.ssh/id_rsa.pub >~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys
cat ~/.ssh/id_rsa.pub
cat ~/.ssh/id_rsa

##########
# AMBARI #
##########

cd /etc/zypp/repos.d
sudo wget http://public-repo-1.hortonworks.com/ambari/suse11/2.x/updates/2.2.2.0/ambari.repo -O /etc/zypp/repos.d/ambari.repo
sudo zypper ref
sudo zypper install ambari-server

sudo /usr/sbin/ambari-server setup
ACCEPT DEFAULTS
sudo /usr/sbin/ambari-server restart

# Install HDP 2.4 with repo 2.4.2 (not 2.4.3 as that is Spark 1.6.2)
# Create Cluster; When selecting key, use the bottom Private one 
# use "cluster_admin" user
# Install HDFS, Yarn, Spark, Ambari Metrics, Hive (needed because of Spark dependency)
# assign EVERYTHING on all slaves and clients

###############
# HIVE CONFIG #
###############

# https://docs.hortonworks.com/HDPDocuments/Ambari-2.2.2.0/bk_ambari_reference_guide/content/_using_hive_with_postgresql.html

# when prompted for hive login and password for hive repo, need to do following steps

# should be root
exit 
zypper install -y postgresql-jdbc
ls /usr/share/java/postgresql-jdbc.jar
chmod 644 /usr/share/java/postgresql-jdbc.jar
ambari-server setup --jdbc-db=postgres --jdbc-driver=/usr/share/java/postgresql-jdbc.jar

su - postgres
psql
create database hive;
create user hive with password 'hive';
grant all privileges on database hive to hive;
\q
# should be root now
exit 
cp /var/lib/pgsql/data/pg_hba.conf /var/lib/pgsql/data/pg_hba.conf_backup
vi /var/lib/pgsql/data/pg_hba.conf
-- add hive to the list of users at the bottom (so hive,mapred,ambari)
:wq!
sudo /etc/init.d/postgresql restart

###################################################
# .. IF YOU WANT TO LOAD SOME TEST DATA INTO HIVE #
###################################################

su - hive
wget https://www.dropbox.com/s/h8b04ej9r2dopa2/SHA_create_employee_table.sql?dl=0 -O SHA_create_employee_table.sql
wget https://www.dropbox.com/s/tj59coispq9kb8g/SHA_Employee.dat?dl=0 -O SHA_Employee.dat

hive -f SHA_create_employee_table.sql
hdfs dfs -mkdir /apps/hive/warehouse/sha.db/
hdfs dfs -put SHA_Employee.dat /apps/hive/warehouse/sha.db/employee
hdfs dfs -ls /apps/hive/warehouse/sha.db/employee
hdfs dfs -chown -R hive:hdfs /apps/hive/warehouse/sha.db

#############
# TEST HIVE #
#############

hive
show databases
use sha
show tables
select * from employee
exit
exit

#############
# TEST HDFS #
#############

exit
su - hdfs
hdfs dfs -mkdir /user/cluster_admin
hdfs dfs -chown cluster_admin /user/cluster_admin
echo "1,2,Hello" > test.csv
hdfs dfs -ls /user/cluster_admin
hdfs dfs -put test.csv /user/cluster_admin
hdfs dfs -ls /user/cluster_admin
hdfs dfs -cat /user/cluster_admin/test.csv
exit

###############
# SETUP SPARK #
###############

sudo su -
sudo vi /etc/bash.bashrc
-- Type G, I (in VI)
Paste ;
## Paths in bash.bashrc FOR AMBARI ##
export JAVA_HOME=/usr/jdk64/jdk1.8.0_60/
export HADOOP_CONF_DIR=/etc/hadoop/conf
export SPARK_HOME=/usr/hdp/2.4.2.0-258/spark
export SPARK_CONF_DIR=$SPARK_HOME/conf
export PATH=$PATH:$SPARK_HOME/bin

# exit to ec2-user and relogin to cluster_admin to take effect

##############
# TEST SPARK #
##############

# http://spark.apache.org/examples.html

spark-shell

spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --num-executors 2 --driver-memory 512m --executor-memory 512m --executor-cores 2 --queue default $SPARK_HOME/lib/spark-examples*.jar 10 2>/dev/null

# should see "Pi is roughly 3.140292"

################
# VORA INSTALL #
################

rm SAPHanaVora*.gz
wget https://www.dropbox.com/s/xxx/VORA_AM1_03P_11-80002420.gz?dl=0 -O SAPHanaVora-ambari.tar.gz
ls /var/lib/ambari-server/resources/stacks/HDP/2.4/services
sudo tar -xvzf SAPHanaVora-ambari.tar.gz -C /var/lib/ambari-server/resources/stacks/HDP/2.4/services
ls /var/lib/ambari-server/resources/stacks/HDP/2.4/services
sudo /usr/sbin/ambari-agent restart
sudo /usr/sbin/ambari-server restart
# make sure all services started
# add vora manager service in ambari 
# for vora manager, if a single node, add vora master, client & worker

# for vora config
ls /usr/hdp/2.4.2.0-258/spark
ls /usr/jdk64/jdk1.8.0_60

###########
# AWS FIX #
###########

# Below is a fix for Vora 1.3.61 in AWS, but will be fixed
# mismatch of address (in aws different internal and external address). 

sudo touch /etc/vora/aws
sudo vi /var/lib/ambari-agent/cache/stacks/HDP/2.4/services/vora-manager/package/scripts/params.py

# Add: 
import socket
#self_host = config['public_hostname']
self_host = socket.getfqdn()
 
# Then save the above file and restart Vora via Ambari.

#################
# PASSWORD FILE #
#################

exit
# find / -name 'genpasswd.sh'
cd /var/lib/ambari-server/resources/stacks/HDP/2.4/services/
./genpasswd.sh

ls /etc/vora/datatools/ -l
chown vora /etc/vora/datatools/htpasswd
chmod 600 /etc/vora/datatools/htpasswd
ls /etc/vora/datatools/ -l
ls /etc/vora/manager/ -l
cp /etc/vora/datatools/htpasswd /etc/vora/manager/
ls /etc/vora/manager/ -l
chown vora /etc/vora/manager/htpasswd
chmod 600 /etc/vora/manager/htpasswd
ls /etc/vora/manager/ -l

##################
# VORA PROCESSES #
##################

ps -ef | grep vora

##########
# CONSUL #
##########

-- to turn on consul
sudo touch /etc/vora/consul_ui
export NOMAD_BIN_DIR=/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/vora-manager/package/lib/vora-scheduler/bin/
$NOMAD_BIN_DIR/nomad status
$NOMAD_BIN_DIR/nomad status vora-dlog
http://vora13:8500/ui/

################
# TO TEST VORA #
################

tail -f /var/log/vora-manager

http://vora13:19000/
http://vora13:9225/

sudo vi /etc/bash.bashrc
# Type G, I (in VI)
Paste ;
## Paths in bash.bashrc FOR AMBARI ##
export VORA_SPARK_HOME=/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/vora-manager/package/lib/vora-spark
# exit and go back in as cluster_admin

ls $VORA_SPARK_HOME
ls $VORA_SPARK_HOME/bin/
$VORA_SPARK_HOME/bin/start-spark-shell.sh --master yarn-client

import org.apache.spark.sql.SapSQLContext

val vc = new SapSQLContext(sc)

vc.sql("show tables").show

val testsql = 
"""
create table testtable (a1 int, a2 int, a3 string)
using com.sap.spark.vora
options (files "/user/cluster_admin/test.csv")
"""

vc.sql(testsql).show

vc.sql("show tables").show

vc.sql("select * from testtable").show

vc.sql("drop table testtable").show

vc.sql("show tables").show

+++++++++++++++
+ DELETE VORA +
+++++++++++++++

curl -u admin:admin -X DELETE -H 'X-Requested-By:admin' http://localhost:8080/api/v1/clusters/SHA/services/HANA_VORA_MANAGER

sudo rm -rf /var/lib/ambari-server/resources/stacks/HDP/2.4/services/vora*
sudo rm -rf /var/lib/ambari-agent/cache/stacks/HDP/2.4/services/vora*
sudo rm -rf /var/log/vora-manager
sudo rm -rf /var/log/vora*
sudo rm -rf /etc/vora/
sudo rm -rf /var/local/vora*
sudo rm -rf /var/run/vora*
sudo rm -rf /lib/vora*
sudo rm -rf /run/lock/vora/
sudo rm -rf /var/lock/vora/
sudo rm -rf /var/log/messages-*

sudo /usr/sbin/ambari-agent restart
sudo /usr/sbin/ambari-server restart

++++++++++++++++++++
+ ZEPPELIN INSTALL +
++++++++++++++++++++

# to find scala version 2.10 uses zep 0.6.0
util.Properties.versionString

http://zeppelin.apache.org/download.html

wget http://archive.apache.org/dist/zeppelin/zeppelin-0.6.0/zeppelin-0.6.0-bin-all.tgz -O zeppelin-0.6.0-bin-all.tgz

tar -xvzf zeppelin-0.6.0-bin-all.tgz

sudo vi /etc/bash.bashrc
-- Type G, I (in VI)
Paste ;
export ZEPPELIN_HOME=/home/cluster_admin/zeppelin-0.6.0-bin-all/
exit
su - cluster_admin

ls /var/lib/ambari-agent/cache/stacks/HDP/2.4/services/vora-manager/package/lib/vora-spark/zeppelin/

ls $ZEPPELIN_HOME/interpreter/spark/

cp /var/lib/ambari-agent/cache/stacks/HDP/2.4/services/vora-manager/package/lib/vora-spark/zeppelin/zeppelin-*.jar $ZEPPELIN_HOME/interpreter/spark/

cd $ZEPPELIN_HOME/interpreter/spark/
jar xf zeppelin-1*.jar interpreter-setting.json
ls
jar uf zeppelin-spark-*.jar interpreter-setting.json
rm interpreter-setting.json

cd $ZEPPELIN_HOME/conf/
cp zeppelin-env.sh.template zeppelin-env.sh
chmod 0755 $ZEPPELIN_HOME/conf/zeppelin-env.sh
sudo vi $ZEPPELIN_HOME/conf/zeppelin-env.sh
export MASTER=yarn-client
export HADOOP_CONF_DIR=/etc/hadoop/conf
export SPARK_HOME=/usr/hdp/2.4.2.0-258/spark

cp $ZEPPELIN_HOME/conf/zeppelin-site.xml.template $ZEPPELIN_HOME/conf/zeppelin-site.xml

chmod 0755 $ZEPPELIN_HOME/conf/zeppelin-site.xml

vi $ZEPPELIN_HOME/conf/zeppelin-site.xml
- / to search
- <name>zeppelin.interpreters</name>
- add <value>,sap.zeppelin.spark.SapSqlInterpreter</value>
- add above interpreter after 1st spark one
- / to search
- <name>zeppelin.server.port</name>
- <value>from 8080 to 9099</value>
- <description>Server port.</description>

ls /usr/hdp/2.4.2.0-258/
In Ambari, select YARN service, choose /Configs/Advanced/Custom yarn-site.
Choose Add Property "hdp.version" and make value "2.4.2.0-258".

$ZEPPELIN_HOME/bin/zeppelin-daemon.sh restart

http://??:9099

ls /var/lib/ambari-agent/cache/stacks/HDP/2.4/services/vora-manager/package/lib/vora-spark/lib/

# paste into dependency
/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/vora-manager/package/lib/vora-spark/lib/spark-sap-datasources-1.3.102-assembly.jar

# make sure "master" is set to yarn-client

%spark.vora show tables

%spark.vora create table testtable (a1 int, a2 int, a3 string)
using com.sap.spark.vora
options (files "/user/cluster_admin/test.csv")

%spark.vora show tables

%spark.vora select * from testtable

%spark.vora drop table testtable

%spark.vora show tables

echo "Sales,10" >> aggdata.csv
echo "HR,20" >> aggdata.csv
echo "Operations,15" >> aggdata.csv
echo "Finance,18" >> aggdata.csv
hdfs dfs -ls /user/cluster_admin
hdfs dfs -put aggdata.csv /user/cluster_admin
hdfs dfs -ls /user/cluster_admin
hdfs dfs -cat /user/cluster_admin/aggdata.csv

%spark.vora create table aggtable (team string, quantity int)
using com.sap.spark.vora
options (files "/user/cluster_admin/aggdata.csv")
