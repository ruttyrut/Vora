++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++
+++ VORA 1.2 INSTALL NOTES +++
++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++

/*
-- Useful Resources --
http://help.sap.com/Download/Multimedia/hana_vora/SAP_HANA_Vora_Installation_Admin_Guide_en.pdf
http://scn.sap.com/community/hana-in-memory/blog/2016/04/25/vora-12-installation-cheat-sheet
http://scn.sap.com/blogs/vora/2015/12/09/sap-hana-vora--troubleshooting
*/

== in AWS ==
Create VPC 
Create 3 Linux Nodes based on SUSE v11 SP3
Optionally create Windows Box
-- See videos 01 (SMP), 02 (VPC), 03 (RDP) & 04 (Nodes) in Vora 1.2 Install Series of videos

-- ON ALL LINUX NODES
sudo su -
echo "10.0.0.??    ip-10-0-0-??.ec2.internal     ip-10-0-0-??" | tee -a /etc/hosts
echo "10.0.0.??    ip-10-0-0-??.ec2.internal     ip-10-0-0-??" | tee -a /etc/hosts
echo "10.0.0.??    ip-10-0-0-??.ec2.internal     ip-10-0-0-??" | tee -a /etc/hosts

/etc/init.d/ntp restart

--- ON MASTER LINUX NODE
chmod 700 /etc/sudoers
echo "%sysadmin ALL=(ALL) NOPASSWD:ALL" | tee -a /etc/sudoers
chmod 440 /etc/sudoers
groupadd sysadmin
sudo /usr/sbin/useradd -m -g users -G sysadmin cluster_admin
sudo -iu cluster_admin
ssh-keygen -t rsa
PRESS RETURN
PRESS RETURN
chmod 700 ~/
chmod 700 ~/.ssh
cat ~/.ssh/id_rsa.pub >~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys
cat ~/.ssh/id_rsa.pub
cat ~/.ssh/id_rsa
-- copy to local file cluster.keys

exit
exit
-- make sure you are ec2-user going forward
id
-- the following folder on Vora Node 1 will contain the Vora Install files from SMP.
mkdir /home/ec2-user/media
cd /home/ec2-user/media
pwd
ls

-- Instead of running the next ~7 lines of scripts, you can instead run 
-- : "zypper install libgcc_s1 libstdc++6" as per the Install Guide.

wget https://www.dropbox.com/s/big19drvsxz236p/SLES11-compat-c%2B%2B.tar?dl=0 -O SLES11-compat-c++.tar
ls
mkdir /home/ec2-user/media/tmp
tar -xf SLES11-compat-c++.tar -C tmp
sudo zypper -n install tmp/*rpm

exit

-- ON SLAVE LINUX NODES
chmod 700 /etc/sudoers
echo "%sysadmin ALL=(ALL) NOPASSWD:ALL" | tee -a /etc/sudoers
chmod 440 /etc/sudoers
groupadd sysadmin
sudo /usr/sbin/useradd -m -g users -G sysadmin cluster_admin
sudo -iu cluster_admin
mkdir ~/.ssh
chmod 700 ~/.ssh

echo "INSERT THE KEY HERE" > ~/.ssh/authorized_keys
-- past in line 2 from cluster.keys
-- like ;
echo "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDE/dntMmsKAwtWMNRxnhAskY3Gnra5K1ecDXwpeZLMrWahfRB60lP88gWG0LsgsvkSPSzK10ucOkRqsh5QoyrajO2NvvmNz/lofC+jUCR/eGqWtoHcf2FtnvC+A3niw6VhAgycFOYFdU45amKfj3mt5kAVZtOM+wfGK+3POj7o8????????????????????????????????????64KtmOqPgtkHXaQITzgrCVBR4mLtruiolPNbkaDWhYoiXGvRSfmJQWc0D6RrV6alC7KcbiHtWfpsCykgZRmDOTFro9WPuFJTXzFmlHFYUQIZGS4/KDXr7AcKlHYBfMVSZ cluster_admin@ip-10-0-0-??" > ~/.ssh/authorized_keys
cat ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys
exit
exit

-- Instead of running the next ~15 lines of scripts, you can instead run 
-- : "zypper install libgcc_s1 libstdc++6" as per the Install Guide.
-- You don't need the /home/ec2-user/media folder if you run the above command on Vora Node 2 and Vora Node 3.

-- make sure you are ec2-user going forward
id
mkdir /home/ec2-user/media
cd /home/ec2-user/media
pwd
ls

wget https://www.dropbox.com/s/big19drvsxz236p/SLES11-compat-c%2B%2B.tar?dl=0 -O SLES11-compat-c++.tar
ls

mkdir /home/ec2-user/media/tmp
tar -xf SLES11-compat-c++.tar -C tmp
sudo zypper -n install tmp/*rpm
exit

+++++++++++++++
+   AMBARI    +
+++++++++++++++

-- BACK ON MASTER

sudo su -
sudo /usr/sbin/useradd -m -g users vora
sudo passwd vora
-- enter password
exit

-- make sure you are cluster_admin
sudo su -
su - cluster_admin
id
cd /etc/zypp/repos.d

sudo wget http://public-repo-1.hortonworks.com/ambari/suse11/2.x/updates/2.2.0.0/ambari.repo -O /etc/zypp/repos.d/ambari.repo
sudo zypper ref
sudo zypper -n install ambari-server
sudo /usr/sbin/ambari-server setup
ACCEPT DEFAULTS
sudo /usr/sbin/ambari-server start

-- Open Port 8080 in CAL, but use Chrome on Windows Image
-- show how to change admin password
-- User HDP 2.3
-- Create Cluster; When selecting key, use the bottom Private one 
-- use "cluster_admin" user
-- Install HDFS, Yarn, Zookeeper, Spark, Ambari Metrics
-- If 3 nodes (a, b & c) HDFS Datanodes to be installed on nodes b & c and HDFS NameNode on node a
-- assign EVERYTHING on all slaves and clients

+++++++++++++++
+  TEST HDFS  +
+++++++++++++++

sudo su -
sudo -iu hdfs
hdfs dfs -mkdir /user/vora
hdfs dfs -chown vora /user/vora
exit
-- validate with ;
sudo -iu vora
echo "1,2,Hello" > test.csv
hdfs dfs -put test.csv
hdfs dfs -cat /user/vora/test.csv
exit

++++++++++++++++
+ SETUP SPARK  +
++++++++++++++++

-- add the following as a "Custom spark-defaults" in the "Configs" tab in Ambari for the Spark service
spark.master yarn-client
-- it means spark won't run on local mode (ie. on this machine only) but will use yarn (on all nodes) instead

++++++++++++++++
+  TEST SPARK  +
++++++++++++++++

sudo su -
su - vora
id
-- should be "vora" user
spark-shell
exit
-- run following out of spark (with the vora user prompt) to test spark ;
spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --num-executors 2 --driver-memory 512m --executor-memory 512m --executor-cores 2 --queue default /usr/hdp/2.3.4.7-4/spark/lib/spark-examples*.jar 10 2>/dev/null
-- should see "Pi is roughly 3.140292"

++++++++++++++++
+ VORA INSTALL +
++++++++++++++++

exit
su - ec2-user
id
cd /home/ec2-user/media
ls

-- see Video 1 for "official" download paths from SAP Service Market Place
-- use VORA_AM1_02_0-70001227.gz for Ambari
wget https://www.dropbox.com/s/?????????/VORA_AM1_02_0-70001227.gz?dl=0 -O VORA_AM1_02_0-70001227.gz

sudo tar -xvzf VORA_AM1_02_0-70001227.gz -C /var/lib/ambari-server/resources/stacks/HDP/2.3/services
sudo /usr/sbin/ambari-server restart
-- make sure all services started. If they haven't, or you see an orange circle with an explanation mark, then run ;
sudo su -
sudo - cluster_admin/vora
sudo /usr/sbin/ambari-agent restart
-- on EACH of your 3 nodes (you can use the vora or cluster_admin user)

-- add vora service in ambari 
-- can install in any order, but best one by one, so install in following order ;
(In our scenario, we have 3 nodes; a, b, c)
1. vora base (install client on all nodes)
-- just a bunch of client libraries
2. discovery server
-- Cannot have client and server on same node. 
-- If 3 nodes, discovery on 3 nodes and no clients needed and no clients should be installed
-- each server can act as a client
-- cannot install client and server on same node, you will have issues, as they use the same port
-- need 3 discovery servers, so 3 nodes is official minimum, although possible to hack and install on 1 (not supported)
-- Vora discovery servers need extra configurations:
---- in vora_discovery_bootstrap add the master DNS (a)
---- add the public dns, not the private one of node a
---- in vora_discovery_servers add your server DNSs (a, b, c)
---- add the public dns, not the private one of nodes a, b, c
3. dlog (on all nodes)
-- dlog is the persistency used by the catalog, so must be before catalog
-- documentation says 5 minimu, but 3 is fine
-- Install DLOG servers on the same machines as Discovery Servers
4. catalog (on node a (master)) 
-- need to install on node that has dlog as it writes to dlog
5. v2 (on nodes b & c (where hdfs datanodes are)
-- is reading files out of hdfs, so want to have datanodes local
6. thrift server (on node a)
- have to add more configurations to the thrift server as below:
- on Vora Node 1, do ;
ls /usr/jdk64/jdk1.8.0_60
- add vora_thriftserver_java_home = /usr/jdk64/jdk1.8.0_60 --this value depends on where JAVA installed on your system
- on Vora Node 1, do ;
ls /usr/hdp/2.3.4.7-4/spark
- add vora_thriftserver_spark_home =  /usr/hdp/2.3.4.7-4/spark --this is your Spark Home value
7. tools (on node a) 
-- thrift and tools on same machine, as they talk to each other

++++++++++++++++
+ TO TEST VORA +
++++++++++++++++

sudo su -
su - vora
id
-- should be vora user

source /etc/vora/vora-env.sh

$VORA_SPARK_HOME/bin/start-spark-shell.sh

import org.apache.spark.sql.SapSQLContext

val vc = new SapSQLContext(sc)

val testsql = """
CREATE TABLE testtable (a1 int, a2 int, a3 string)
USING com.sap.spark.vora
OPTIONS (
tableName "testtable",
paths "/user/vora/test.csv"
)"""

vc.sql(testsql).show

vc.sql("show tables").show

vc.sql("describe testtable").show

vc.sql("select * from testtable").show

++++++++++++++++++
+ TO DELETE VORA +
++++++++++++++++++

-- stop all servers

sudo su -

su - cluster_admin

-- note that for the following admin:Welcome1 is the login and password for ambari, 
-- so you may need to change admin password (Welcome1) to your changed password

curl -u admin:Welcome1 -X DELETE -H 'X-Requested-By:admin' http://localhost:8080/api/v1/clusters/SHA/services/HANA_VORA_BASE

curl -u admin:Welcome1 -X DELETE -H 'X-Requested-By:admin' http://localhost:8080/api/v1/clusters/SHA/services/HANA_VORA_CATALOG

curl -u admin:Welcome1 -X DELETE -H 'X-Requested-By:admin' http://localhost:8080/api/v1/clusters/SHA/services/HANA_VORA_DISCOVERY

curl -u admin:Welcome1 -X DELETE -H 'X-Requested-By:admin' http://localhost:8080/api/v1/clusters/SHA/services/HANA_VORA_DLOG

curl -u admin:Welcome1 -X DELETE -H 'X-Requested-By:admin' http://localhost:8080/api/v1/clusters/SHA/services/HANA_VORA_THRIFTSERVER

curl -u admin:Welcome1 -X DELETE -H 'X-Requested-By:admin' http://localhost:8080/api/v1/clusters/SHA/services/HANA_VORA_TOOLS

curl -u admin:Welcome1 -X DELETE -H 'X-Requested-By:admin' http://localhost:8080/api/v1/clusters/SHA/services/HANA_VORA_V2SERVER

-- delete vora folders/services on Vora Node 1 in ;
sudo rm -rf /var/lib/ambari-server/resources/stacks/HDP/2.3/services/vora*
sudo rm -rf /home/vora/metastore_db
sudo rm -rf /home/vora/derby.log

-- delete vora logs (to be run on all nodes) in ;
sudo rm -rf /var/log/vora*

-- delete vora content (to be run on all nodes) in ;
sudo rm -rf /etc/vora/
sudo rm -rf /var/local/vora*

sudo /usr/sbin/ambari-server restart
